# -*- coding: utf-8 -*-
"""california_housing_subclassing_api

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HTa2pduo70dPfUJUh2Ocse828pVQvT7K
"""

import tensorflow as tf
from tensorflow import keras

import tensorflow as tf
from tensorflow import keras

class Linear(keras.layers.Layer):
  def __init__(self, units=32 , input_dim=32):
    super(Linear,self).__init__()
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value = w_init(shape=(input_dim,units),dtype="float32"),
        trainable=True
    )
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value = b_init(shape=(units,), dtype="float32"),
        trainable=True

    )
  def call(self , inputs):
    print("Call method is being executed!") # Added for debugging
    return tf.matmul(inputs , self.w) +self.b

#oluşturduğumuz katmanın nasıl çalıştığını görmek için bir örnek
x=tf.ones((2,2))
linear_layer = Linear(4,2)
y=linear_layer(x)
print(y)

linear_layer.weights

#add_weight ile katman oluşturma

from tensorflow.python.ops.init_ops_v2 import Initializer
class Linear(keras.layers.Layer):
  def __init__(self,units=32 , input_dim=32):
    super(Linear,self).__init__()
    self.w = self.add_weight(shape=(input_dim , units),
                             initializer = "random_normal",
                             trainable= True)

    self.b = self.add_weight(shape=(units,),
                             initializer = "zeros",
                             trainable=True)

  def call(self , inputs):
    return tf.matmul(inputs , self.w) + self.b

#örnek verilerimiz ile oluşturduğumuz katman nasıl çalışıyor bakalım
x=tf.ones((2,2))
linear_layer = Linear(4,2)
y=linear_layer(x)
print(y)

#girdi boyutu yazmadan katmanlar
class Linear(keras.layers.Layer):
  def __init__(self,units=32):
    super(Linear,self).__init__()
    self.units = units
  def build(self,input_shape):
    self.w = self.add_weight(shape=(input_shape[-1],self.units),
                             initializer = "random_normal",
                             trainable= True)
    self.b = self.add_weight(shape=(self.units,),
                             initializer = "zeros",
                             trainable=True)
  def call(self , inputs):
    return tf.matmul(inputs , self.w) + self.b

#katmandan örnek
linear_layer  = Linear(32)
y=linear_layer(x)
print(y)

#ard arda gelen katmanlar
class MLPBlock(keras.layers.Layer):
  def __init__(self):
    super(MLPBlock,self).__init__()
    self.linear_1 = Linear(32)
    self.linear_2 = Linear(32)
    self.linear_3 = Linear(1)
  def call(self,inputs):
    x = self.linear_1(inputs)
    x=tf.nn.relu(x)
    x=self.linear_2(x)
    x=tf.nn.relu(x)
    x=self.linear_3(x)
    x=tf.nn.relu(x)
    return x # Corrected: Removed the duplicate call to self.linear_3

#bu sınıftan bir örnek yapalım
mlp = MLPBlock()
y=mlp(tf.ones(shape=(3,64)))
y

#MODEL KURMA

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

for i in housing:
  print(i)

from sklearn.model_selection import train_test_split
X_train_full , X_test , y_train_full , y_test = train_test_split(housing.data , housing.target ,random_state=42)
X_train , X_valid , y_train , y_valid = train_test_split(
    X_train_full , y_train_full , random_state=42
)

import tensorflow as tf
from tensorflow import keras

#önce katmanları olusturalım sonra sinir ağını kuralım.
class WideAndDeepModel(tf.keras.Model):
  def __init__(self,units=30,activation="relu",**kwargs):
    super().__init__(**kwargs)
    self.norm_layer_wide=tf.keras.layers.Normalization()
    self.norm_layer_deep=tf.keras.layers.Normalization()
    self.hidden1 = tf.keras.layers.Dense(units,activation=activation)
    self.hidden2 = tf.keras.layers.Dense(units,activation=activation)
    self.main_output = tf.keras.layers.Dense(1)

  def call(self,inputs):
    input_wide , input_deep = inputs
    norm_wide = self.norm_layer_wide(input_wide)
    norm_deep = self.norm_layer_deep(input_deep)
    hidden1 = self.hidden1(norm_deep)
    hidden2 = self.hidden2(hidden1)
    concat = tf.keras.layers.concatenate([norm_wide,hidden2])
    output = self.main_output(concat)
    return output

#örnek veri ile test edelim
tf.random.set_seed(42)
model = WideAndDeepModel(30,activation="relu" , name="my_model")

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss="mse",optimizer=optimizer,metrics=["RootMeanSquaredError"])

X_train_wide , X_train_deep = X_train[: , :5] , X_train[: , 2:]
X_valid_wide , X_valid_deep = X_valid[: , :5] , X_valid[: , 2:]
X_test_wide , X_test_deep = X_test[: , :5] , X_test[: , 2:]

#modeli eğitelim
model.norm_layer_deep.adapt(X_train_deep)
model.norm_layer_wide.adapt(X_train_wide)

history = model.fit((X_train_wide,X_train_deep),y_train,
                    validation_data=((X_valid_wide,X_valid_deep),y_valid),
                    epochs=10)

#değerlendirelim
eval_result = model.evaluate((X_test_wide,X_test_deep),y_test)
eval_result

from re import X
#yeni veriler ile tahmin edelim
X_new_wide , X_new_deep = X_test_wide[:3] , X_test_deep[:3]
y_pred = model.predict((X_new_wide,X_new_deep))
y_pred

y_test[:3]

